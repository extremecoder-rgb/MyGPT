{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3acbbf66",
   "metadata": {},
   "source": [
    "I am building my own LLM so first I am building a tokenizer of my own "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c0e579",
   "metadata": {},
   "source": [
    "STEP 1 - CREATE THE TOKENS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d4f192a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "439478\n",
      "M r. and Mrs. Dursley, of number four, Privet Drive, were proud to say that they were perfectly nor\n"
     ]
    }
   ],
   "source": [
    "with open(\"01 Harry Potter and the Sorcerers Stone.txt\", \"r\", encoding=\"Utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "\n",
    "print(len(raw_text)) #Total number of characters\n",
    "print(raw_text[:99])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "573c7f0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['M', 'r', '.', 'and', 'Mrs', '.', 'Dursley', ',', 'of', 'number', 'four', ',', 'Privet', 'Drive', ',', 'were', 'proud', 'to', 'say', 'that', 'they', 'were', 'perfectly', 'normal', ',', 'thank', 'you', 'very', 'much', '.']\n"
     ]
    }
   ],
   "source": [
    "import re  #library for splitting\n",
    "preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', raw_text)\n",
    "preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "print(preprocessed[:30]) # Remember this is a list having all the tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "680d5641",
   "metadata": {},
   "source": [
    "STEP 2 - CREATE TOKEN ID FOR THIS TOKENS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3565b607",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7572\n"
     ]
    }
   ],
   "source": [
    "#now we have to use vocabulary that is the sorted words\n",
    "words = sorted(set(preprocessed))\n",
    "vocab_size = len(words)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6aeb2f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remember vocab is a mapping from tokens to tokenIDs\n",
    "vocab = {token:integer for integer, token in enumerate(words)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed5f7af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('!', 0)\n",
      "(\"'\", 1)\n",
      "('(', 2)\n",
      "(')', 3)\n",
      "('*', 4)\n",
      "(',', 5)\n",
      "('-', 6)\n",
      "('-bodied', 7)\n",
      "('.', 8)\n",
      "('1', 9)\n",
      "('1473', 10)\n",
      "('1637', 11)\n",
      "('17', 12)\n",
      "('1709', 13)\n",
      "('1945', 14)\n",
      "('2', 15)\n",
      "('3', 16)\n",
      "('31', 17)\n",
      "('382', 18)\n",
      "('4', 19)\n",
      "(':', 20)\n",
      "(';', 21)\n",
      "('?', 22)\n",
      "('A', 23)\n",
      "('ALBUS', 24)\n",
      "('ALLEY', 25)\n",
      "('ALLOWED', 26)\n",
      "('AM', 27)\n",
      "('AND', 28)\n",
      "('ANYTHING', 29)\n",
      "('ARE', 30)\n",
      "('AT', 31)\n",
      "('About', 32)\n",
      "('According', 33)\n",
      "('Adalbert', 34)\n",
      "('Add', 35)\n",
      "('Adrian', 36)\n",
      "('Africa', 37)\n",
      "('African', 38)\n",
      "('After', 39)\n",
      "('Against', 40)\n",
      "('Ages', 41)\n",
      "('Agrippa', 42)\n",
      "('Ah', 43)\n",
      "('Ahead', 44)\n",
      "('Alberic', 45)\n",
      "('Albus', 46)\n",
      "('Albus…”', 47)\n",
      "('Algie', 48)\n",
      "('Alicia', 49)\n",
      "('All', 50)\n"
     ]
    }
   ],
   "source": [
    "#enumerate - this I have used to assign integer values to the sorted words\n",
    "for i, item in enumerate(vocab.items()):\n",
    "    print(item)\n",
    "    if i>=50:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c31949",
   "metadata": {},
   "source": [
    "WE NEED TO REMEMBER TWO CONCEPTS i.e. ENCODER AND DECODER\n",
    "\n",
    "ENCODER -> it will take text as input and give token ID as output\n",
    "\n",
    "DECODER -> it will take token ID as input and will give text as output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bd5e32b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizer:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = {i:s for s,i in vocab.items()}\n",
    "\n",
    "    def encode(self, text):\n",
    "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "        ids = [self.str_to_int[s] for s in preprocessed]\n",
    "        return ids\n",
    "    \n",
    "    def decode(self, ids):\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8e76bf6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[593, 6597, 4470, 6131, 2269, 4452, 6131, 5888, 6126, 3488, 4427]\n"
     ]
    }
   ],
   "source": [
    "#Now let us create an object of the above class and check if it is returning the ids or not\n",
    "tokenizer = SimpleTokenizer(vocab)\n",
    "text =  \"\"\"It was on the corner of the street that he noticed\"\"\"\n",
    "ids = tokenizer.encode(text)\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b714b33d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'It was on the corner of the street that he noticed'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a10f8e8f",
   "metadata": {},
   "source": [
    "So encoder and decoder is perfectly working as expected"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f5ffb6d",
   "metadata": {},
   "source": [
    "NOW WE HAVE COMPLETED A SIMPLE TOKENIZER OF OUR OWN BUT THERE ARE SOME LIMITATIONS. WE CANNOT USE OTHER TEXT OTHER THAN THE ONE IN THE txt file WE USED MEANING IF I WRITE SOME TEXT THERE WILL BE ERROR SO WE NEED SPECIAL CONTEXT TOKENS WHICH IS ALSO USED BY GPT MODELS .\n",
    "\n",
    "<|endofText|> and <|unk|> -> unknown will be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed60075",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7574"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = sorted(list(set(preprocessed)))\n",
    "tokens.extend([\"<|endofText|>\", \"<|unk|>\"])# Here I used extend it will add 2 extra entries see the result it is 7574 but previously it was 7572\n",
    "vocab = {token:integer for integer, token in enumerate(tokens)}\n",
    "len(vocab.items())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40cc2815",
   "metadata": {},
   "source": [
    "Let us check "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "94ef89f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('…Oak', 7569)\n",
      "('…Then', 7570)\n",
      "('…”', 7571)\n",
      "('<|endofText|>', 7572)\n",
      "('<|unk|>', 7573)\n"
     ]
    }
   ],
   "source": [
    "for i, item in enumerate(list(vocab.items())[-5:]):\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7789a176",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdvancedTokenizer:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = {i:s for s,i in vocab.items()}\n",
    "\n",
    "    def encode(self,text):\n",
    "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "        preprocessed = [item if item in self.str_to_int else \"<|unk|>\" for item in preprocessed]\n",
    "        ids = [self.str_to_int[s] for s in preprocessed]\n",
    "        return ids\n",
    "    \n",
    "    def decode(self, ids):\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7743c899",
   "metadata": {},
   "source": [
    "Now let us try again with some random text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "649f76ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, my name is Subhranil Mondal <|endofText|> I am from Kolkata.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AdvancedTokenizer(vocab)\n",
    "\n",
    "text1 = \"Hello, my name is Subhranil Mondal\"\n",
    "text2 = \"I am from Kolkata.\"\n",
    "\n",
    "text = \" <|endofText|> \".join((text1,text2))\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "eecb8b0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[7573, 5, 4318, 4332, 3758, 7573, 7573, 7572, 575, 1391, 3164, 7573, 8]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "98eb0bf4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|unk|>, my name is <|unk|> <|unk|> <|endofText|> I am from <|unk|>.'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenizer.encode(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ac3d5b3",
   "metadata": {},
   "source": [
    "See this is a word based tokenizer but GPT uses subword tokenizer so for that we need Byte Pair encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ad115da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tiktoken\n",
      "  Using cached tiktoken-0.11.0-cp312-cp312-win_amd64.whl.metadata (6.9 kB)\n",
      "Collecting regex>=2022.1.18 (from tiktoken)\n",
      "  Downloading regex-2025.9.18-cp312-cp312-win_amd64.whl.metadata (41 kB)\n",
      "     ---------------------------------------- 0.0/41.5 kB ? eta -:--:--\n",
      "     ---------------------------------------- 41.5/41.5 kB 1.0 MB/s eta 0:00:00\n",
      "Collecting requests>=2.26.0 (from tiktoken)\n",
      "  Using cached requests-2.32.5-py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting charset_normalizer<4,>=2 (from requests>=2.26.0->tiktoken)\n",
      "  Using cached charset_normalizer-3.4.3-cp312-cp312-win_amd64.whl.metadata (37 kB)\n",
      "Collecting idna<4,>=2.5 (from requests>=2.26.0->tiktoken)\n",
      "  Using cached idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests>=2.26.0->tiktoken)\n",
      "  Using cached urllib3-2.5.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests>=2.26.0->tiktoken)\n",
      "  Using cached certifi-2025.8.3-py3-none-any.whl.metadata (2.4 kB)\n",
      "Using cached tiktoken-0.11.0-cp312-cp312-win_amd64.whl (884 kB)\n",
      "Downloading regex-2025.9.18-cp312-cp312-win_amd64.whl (275 kB)\n",
      "   ---------------------------------------- 0.0/275.5 kB ? eta -:--:--\n",
      "   ------------------------- -------------- 174.1/275.5 kB 5.3 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 256.0/275.5 kB 3.2 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 256.0/275.5 kB 3.2 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 256.0/275.5 kB 3.2 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 256.0/275.5 kB 3.2 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 256.0/275.5 kB 3.2 MB/s eta 0:00:01\n",
      "   ------------------------------------ - 266.2/275.5 kB 820.5 kB/s eta 0:00:01\n",
      "   -------------------------------------- 275.5/275.5 kB 808.7 kB/s eta 0:00:00\n",
      "Using cached requests-2.32.5-py3-none-any.whl (64 kB)\n",
      "Using cached certifi-2025.8.3-py3-none-any.whl (161 kB)\n",
      "Using cached charset_normalizer-3.4.3-cp312-cp312-win_amd64.whl (107 kB)\n",
      "Using cached idna-3.10-py3-none-any.whl (70 kB)\n",
      "Using cached urllib3-2.5.0-py3-none-any.whl (129 kB)\n",
      "Installing collected packages: urllib3, regex, idna, charset_normalizer, certifi, requests, tiktoken\n",
      "Successfully installed certifi-2025.8.3 charset_normalizer-3.4.3 idna-3.10 regex-2025.9.18 requests-2.32.5 tiktoken-0.11.0 urllib3-2.5.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "#we will be using the BPE tokenizer which OpenAI uses\n",
    "! pip3 install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc29e1d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0c006a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fcea8358",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15496, 11, 616, 1438, 318, 3834, 16848, 346, 27328, 282, 13, 314, 716, 422, 509, 13597, 1045, 13]\n"
     ]
    }
   ],
   "source": [
    "#Let us test\n",
    "\n",
    "text = \"Hello, my name is Subhranil Mondal. I am from Kolkata.\"\n",
    "integers = tokenizer.encode(text, allowed_special={\"<|endofText|>\"})\n",
    "print(integers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "43fdd490",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, my name is Subhranil Mondal. I am from Kolkata.\n"
     ]
    }
   ],
   "source": [
    "s = tokenizer.decode(integers)\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46533c8f",
   "metadata": {},
   "source": [
    "Therefore encoder and decoder is working pretty well"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa711ab",
   "metadata": {},
   "source": [
    "Now, we have to create Input-Target pairs\n",
    "But what is it?\n",
    "see we need to do self supervised learning kind of unsupervised. The data are not labeled here . LLM will be feed one word and it will predict next one This is what we gonna implement which the GPT does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5f9941da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "124336\n"
     ]
    }
   ],
   "source": [
    "with open(\"01 Harry Potter and the Sorcerers Stone.txt\", \"r\", encoding=\"Utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "\n",
    "encoded_text = tokenizer.encode(raw_text)\n",
    "print(len(encoded_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7eafa68b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = encoded_text[50:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "65bf5d7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: [11428, 11, 780, 484]\n",
      "y: [11, 780, 484, 655]\n"
     ]
    }
   ],
   "source": [
    "conSize = 4 #context size -> length of input sequence\n",
    "x = sample[:conSize]\n",
    "y = sample[1:conSize+1]\n",
    "print(f\"x: {x}\")\n",
    "print(f\"y: {y}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c77846c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11428] -> 11\n",
      "[11428, 11] -> 780\n",
      "[11428, 11, 780] -> 484\n",
      "[11428, 11, 780, 484] -> 655\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, conSize+1):\n",
    "    context = sample[:i]\n",
    "    desired = sample[i]\n",
    "\n",
    "    print(context, \"->\", desired)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd311566",
   "metadata": {},
   "source": [
    "Now just decode it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ec7b11f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " mysterious -> ,\n",
      " mysterious, ->  because\n",
      " mysterious, because ->  they\n",
      " mysterious, because they ->  just\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, conSize+1):\n",
    "    context = sample[:i]\n",
    "    desired = sample[i]\n",
    "\n",
    "    print(tokenizer.decode(context), \"->\", tokenizer.decode([desired]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d0b53d3",
   "metadata": {},
   "source": [
    "so now we have to turn tokens into embeddings and implement a data loader as inputs and PyTorch Tensors as target like you can think about multidimensional arrays."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "366e7dcd",
   "metadata": {},
   "source": [
    "STEP 3 - IMPLEMENT A DATALOADER(WE WILL BE USING PYTORCH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9934607a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Using cached torch-2.8.0-cp312-cp312-win_amd64.whl.metadata (30 kB)\n",
      "Collecting torchvision\n",
      "  Using cached torchvision-0.23.0-cp312-cp312-win_amd64.whl.metadata (6.1 kB)\n",
      "Collecting torchaudio\n",
      "  Using cached torchaudio-2.8.0-cp312-cp312-win_amd64.whl.metadata (7.2 kB)\n",
      "Collecting filelock (from torch)\n",
      "  Using cached filelock-3.19.1-py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting typing-extensions>=4.10.0 (from torch)\n",
      "  Using cached typing_extensions-4.15.0-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting sympy>=1.13.3 (from torch)\n",
      "  Using cached sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting networkx (from torch)\n",
      "  Using cached networkx-3.5-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting jinja2 (from torch)\n",
      "  Using cached jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting fsspec (from torch)\n",
      "  Downloading fsspec-2025.9.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting setuptools (from torch)\n",
      "  Using cached setuptools-80.9.0-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting numpy (from torchvision)\n",
      "  Downloading numpy-2.3.3-cp312-cp312-win_amd64.whl.metadata (60 kB)\n",
      "     ---------------------------------------- 0.0/60.9 kB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/60.9 kB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/60.9 kB ? eta -:--:--\n",
      "     ------ --------------------------------- 10.2/60.9 kB ? eta -:--:--\n",
      "     ------ --------------------------------- 10.2/60.9 kB ? eta -:--:--\n",
      "     ------------------- ------------------ 30.7/60.9 kB 262.6 kB/s eta 0:00:01\n",
      "     ------------------- ------------------ 30.7/60.9 kB 262.6 kB/s eta 0:00:01\n",
      "     ------------------- ------------------ 30.7/60.9 kB 262.6 kB/s eta 0:00:01\n",
      "     ------------------------------- ------ 51.2/60.9 kB 201.8 kB/s eta 0:00:01\n",
      "     ------------------------------- ------ 51.2/60.9 kB 201.8 kB/s eta 0:00:01\n",
      "     -------------------------------------- 60.9/60.9 kB 171.3 kB/s eta 0:00:00\n",
      "Collecting pillow!=8.3.*,>=5.3.0 (from torchvision)\n",
      "  Using cached pillow-11.3.0-cp312-cp312-win_amd64.whl.metadata (9.2 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy>=1.13.3->torch)\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2->torch)\n",
      "  Using cached MarkupSafe-3.0.2-cp312-cp312-win_amd64.whl.metadata (4.1 kB)\n",
      "Using cached torch-2.8.0-cp312-cp312-win_amd64.whl (241.3 MB)\n",
      "Using cached torchvision-0.23.0-cp312-cp312-win_amd64.whl (1.6 MB)\n",
      "Using cached torchaudio-2.8.0-cp312-cp312-win_amd64.whl (2.5 MB)\n",
      "Using cached pillow-11.3.0-cp312-cp312-win_amd64.whl (7.0 MB)\n",
      "Using cached sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "Using cached typing_extensions-4.15.0-py3-none-any.whl (44 kB)\n",
      "Using cached filelock-3.19.1-py3-none-any.whl (15 kB)\n",
      "Downloading fsspec-2025.9.0-py3-none-any.whl (199 kB)\n",
      "   ---------------------------------------- 0.0/199.3 kB ? eta -:--:--\n",
      "   -------- ------------------------------- 41.0/199.3 kB ? eta -:--:--\n",
      "   ------------------------ --------------- 122.9/199.3 kB 1.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 199.3/199.3 kB 1.5 MB/s eta 0:00:00\n",
      "Using cached jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
      "Using cached networkx-3.5-py3-none-any.whl (2.0 MB)\n",
      "Downloading numpy-2.3.3-cp312-cp312-win_amd64.whl (12.8 MB)\n",
      "   ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.2/12.8 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.2/12.8 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.2/12.8 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.2/12.8 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.2/12.8 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.2/12.8 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.2/12.8 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.2/12.8 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.2/12.8 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.2/12.8 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 0.7/12.8 MB 1.3 MB/s eta 0:00:10\n",
      "   -- ------------------------------------- 0.7/12.8 MB 1.4 MB/s eta 0:00:09\n",
      "   -- ------------------------------------- 0.7/12.8 MB 1.4 MB/s eta 0:00:09\n",
      "   -- ------------------------------------- 0.7/12.8 MB 1.4 MB/s eta 0:00:09\n",
      "   -- ------------------------------------- 0.7/12.8 MB 1.4 MB/s eta 0:00:09\n",
      "   -- ------------------------------------- 0.8/12.8 MB 1.1 MB/s eta 0:00:11\n",
      "   -- ------------------------------------- 0.8/12.8 MB 1.1 MB/s eta 0:00:11\n",
      "   -- ------------------------------------- 0.8/12.8 MB 1.1 MB/s eta 0:00:11\n",
      "   -- ------------------------------------- 0.8/12.8 MB 1.1 MB/s eta 0:00:11\n",
      "   -- ------------------------------------- 0.8/12.8 MB 1.1 MB/s eta 0:00:11\n",
      "   -- ------------------------------------- 0.9/12.8 MB 977.8 kB/s eta 0:00:13\n",
      "   -- ------------------------------------- 0.9/12.8 MB 977.8 kB/s eta 0:00:13\n",
      "   -- ------------------------------------- 0.9/12.8 MB 977.8 kB/s eta 0:00:13\n",
      "   --- ------------------------------------ 1.0/12.8 MB 868.0 kB/s eta 0:00:14\n",
      "   --- ------------------------------------ 1.1/12.8 MB 928.4 kB/s eta 0:00:13\n",
      "   --- ------------------------------------ 1.1/12.8 MB 928.4 kB/s eta 0:00:13\n",
      "   --- ------------------------------------ 1.2/12.8 MB 966.8 kB/s eta 0:00:12\n",
      "   --- ------------------------------------ 1.2/12.8 MB 966.8 kB/s eta 0:00:12\n",
      "   --- ------------------------------------ 1.2/12.8 MB 966.8 kB/s eta 0:00:12\n",
      "   --- ------------------------------------ 1.2/12.8 MB 966.8 kB/s eta 0:00:12\n",
      "   ---- ----------------------------------- 1.3/12.8 MB 921.7 kB/s eta 0:00:13\n",
      "   ---- ----------------------------------- 1.3/12.8 MB 921.7 kB/s eta 0:00:13\n",
      "   ---- ----------------------------------- 1.3/12.8 MB 921.7 kB/s eta 0:00:13\n",
      "   ---- ----------------------------------- 1.3/12.8 MB 921.7 kB/s eta 0:00:13\n",
      "   ---- ----------------------------------- 1.4/12.8 MB 825.5 kB/s eta 0:00:14\n",
      "   ---- ----------------------------------- 1.5/12.8 MB 894.8 kB/s eta 0:00:13\n",
      "   ---- ----------------------------------- 1.5/12.8 MB 881.9 kB/s eta 0:00:13\n",
      "   ---- ----------------------------------- 1.5/12.8 MB 881.9 kB/s eta 0:00:13\n",
      "   ---- ----------------------------------- 1.5/12.8 MB 881.9 kB/s eta 0:00:13\n",
      "   ----- ---------------------------------- 1.6/12.8 MB 873.9 kB/s eta 0:00:13\n",
      "   ----- ---------------------------------- 1.6/12.8 MB 873.9 kB/s eta 0:00:13\n",
      "   ----- ---------------------------------- 1.6/12.8 MB 873.9 kB/s eta 0:00:13\n",
      "   ----- ---------------------------------- 1.8/12.8 MB 882.6 kB/s eta 0:00:13\n",
      "   ----- ---------------------------------- 1.8/12.8 MB 880.6 kB/s eta 0:00:13\n",
      "   ------ --------------------------------- 1.9/12.8 MB 924.7 kB/s eta 0:00:12\n",
      "   ------ --------------------------------- 2.1/12.8 MB 968.6 kB/s eta 0:00:12\n",
      "   ------ --------------------------------- 2.1/12.8 MB 968.6 kB/s eta 0:00:12\n",
      "   ------ --------------------------------- 2.1/12.8 MB 950.5 kB/s eta 0:00:12\n",
      "   ------ --------------------------------- 2.2/12.8 MB 976.5 kB/s eta 0:00:11\n",
      "   ------- -------------------------------- 2.3/12.8 MB 987.6 kB/s eta 0:00:11\n",
      "   ------- -------------------------------- 2.3/12.8 MB 983.4 kB/s eta 0:00:11\n",
      "   ------- -------------------------------- 2.4/12.8 MB 1.0 MB/s eta 0:00:11\n",
      "   ------- -------------------------------- 2.5/12.8 MB 1.0 MB/s eta 0:00:11\n",
      "   ------- -------------------------------- 2.5/12.8 MB 1.0 MB/s eta 0:00:11\n",
      "   -------- ------------------------------- 2.6/12.8 MB 1.0 MB/s eta 0:00:11\n",
      "   -------- ------------------------------- 2.7/12.8 MB 1.0 MB/s eta 0:00:10\n",
      "   -------- ------------------------------- 2.7/12.8 MB 1.0 MB/s eta 0:00:10\n",
      "   -------- ------------------------------- 2.8/12.8 MB 1.0 MB/s eta 0:00:10\n",
      "   -------- ------------------------------- 2.8/12.8 MB 1.0 MB/s eta 0:00:10\n",
      "   --------- ------------------------------ 2.9/12.8 MB 1.0 MB/s eta 0:00:10\n",
      "   --------- ------------------------------ 3.0/12.8 MB 1.0 MB/s eta 0:00:10\n",
      "   --------- ------------------------------ 3.0/12.8 MB 1.0 MB/s eta 0:00:10\n",
      "   --------- ------------------------------ 3.1/12.8 MB 1.1 MB/s eta 0:00:10\n",
      "   --------- ------------------------------ 3.1/12.8 MB 1.1 MB/s eta 0:00:10\n",
      "   ---------- ----------------------------- 3.2/12.8 MB 1.1 MB/s eta 0:00:10\n",
      "   ---------- ----------------------------- 3.2/12.8 MB 1.1 MB/s eta 0:00:10\n",
      "   ---------- ----------------------------- 3.3/12.8 MB 1.1 MB/s eta 0:00:09\n",
      "   ---------- ----------------------------- 3.4/12.8 MB 1.1 MB/s eta 0:00:09\n",
      "   ---------- ----------------------------- 3.4/12.8 MB 1.1 MB/s eta 0:00:09\n",
      "   ---------- ----------------------------- 3.4/12.8 MB 1.1 MB/s eta 0:00:09\n",
      "   ---------- ----------------------------- 3.5/12.8 MB 1.1 MB/s eta 0:00:09\n",
      "   ----------- ---------------------------- 3.5/12.8 MB 1.1 MB/s eta 0:00:09\n",
      "   ----------- ---------------------------- 3.6/12.8 MB 1.1 MB/s eta 0:00:09\n",
      "   ----------- ---------------------------- 3.7/12.8 MB 1.1 MB/s eta 0:00:09\n",
      "   ----------- ---------------------------- 3.7/12.8 MB 1.1 MB/s eta 0:00:09\n",
      "   ----------- ---------------------------- 3.8/12.8 MB 1.1 MB/s eta 0:00:09\n",
      "   ------------ --------------------------- 3.9/12.8 MB 1.1 MB/s eta 0:00:09\n",
      "   ------------ --------------------------- 3.9/12.8 MB 1.1 MB/s eta 0:00:09\n",
      "   ------------ --------------------------- 4.0/12.8 MB 1.1 MB/s eta 0:00:08\n",
      "   ------------ --------------------------- 4.1/12.8 MB 1.1 MB/s eta 0:00:08\n",
      "   ------------ --------------------------- 4.1/12.8 MB 1.1 MB/s eta 0:00:08\n",
      "   ------------- -------------------------- 4.2/12.8 MB 1.1 MB/s eta 0:00:08\n",
      "   ------------- -------------------------- 4.2/12.8 MB 1.1 MB/s eta 0:00:08\n",
      "   ------------- -------------------------- 4.3/12.8 MB 1.1 MB/s eta 0:00:08\n",
      "   ------------- -------------------------- 4.4/12.8 MB 1.1 MB/s eta 0:00:08\n",
      "   ------------- -------------------------- 4.4/12.8 MB 1.1 MB/s eta 0:00:08\n",
      "   ------------- -------------------------- 4.5/12.8 MB 1.1 MB/s eta 0:00:08\n",
      "   -------------- ------------------------- 4.5/12.8 MB 1.1 MB/s eta 0:00:08\n",
      "   -------------- ------------------------- 4.6/12.8 MB 1.1 MB/s eta 0:00:08\n",
      "   -------------- ------------------------- 4.6/12.8 MB 1.1 MB/s eta 0:00:08\n",
      "   -------------- ------------------------- 4.7/12.8 MB 1.1 MB/s eta 0:00:08\n",
      "   -------------- ------------------------- 4.8/12.8 MB 1.1 MB/s eta 0:00:08\n",
      "   --------------- ------------------------ 4.9/12.8 MB 1.1 MB/s eta 0:00:08\n",
      "   --------------- ------------------------ 4.9/12.8 MB 1.1 MB/s eta 0:00:07\n",
      "   --------------- ------------------------ 5.0/12.8 MB 1.1 MB/s eta 0:00:07\n",
      "   --------------- ------------------------ 5.0/12.8 MB 1.1 MB/s eta 0:00:07\n",
      "   --------------- ------------------------ 5.1/12.8 MB 1.1 MB/s eta 0:00:07\n",
      "   ---------------- ----------------------- 5.2/12.8 MB 1.1 MB/s eta 0:00:07\n",
      "   ---------------- ----------------------- 5.2/12.8 MB 1.1 MB/s eta 0:00:07\n",
      "   ---------------- ----------------------- 5.3/12.8 MB 1.1 MB/s eta 0:00:07\n",
      "   ---------------- ----------------------- 5.3/12.8 MB 1.1 MB/s eta 0:00:07\n",
      "   ---------------- ----------------------- 5.4/12.8 MB 1.1 MB/s eta 0:00:07\n",
      "   ---------------- ----------------------- 5.4/12.8 MB 1.1 MB/s eta 0:00:07\n",
      "   ----------------- ---------------------- 5.5/12.8 MB 1.1 MB/s eta 0:00:07\n",
      "   ----------------- ---------------------- 5.6/12.8 MB 1.1 MB/s eta 0:00:07\n",
      "   ----------------- ---------------------- 5.6/12.8 MB 1.1 MB/s eta 0:00:07\n",
      "   ----------------- ---------------------- 5.7/12.8 MB 1.2 MB/s eta 0:00:07\n",
      "   ----------------- ---------------------- 5.7/12.8 MB 1.2 MB/s eta 0:00:07\n",
      "   ------------------ --------------------- 5.8/12.8 MB 1.1 MB/s eta 0:00:07\n",
      "   ------------------ --------------------- 5.8/12.8 MB 1.1 MB/s eta 0:00:07\n",
      "   ------------------ --------------------- 5.9/12.8 MB 1.1 MB/s eta 0:00:06\n",
      "   ------------------ --------------------- 6.0/12.8 MB 1.2 MB/s eta 0:00:06\n",
      "   ------------------ --------------------- 6.1/12.8 MB 1.2 MB/s eta 0:00:06\n",
      "   ------------------- -------------------- 6.1/12.8 MB 1.2 MB/s eta 0:00:06\n",
      "   ------------------- -------------------- 6.2/12.8 MB 1.2 MB/s eta 0:00:06\n",
      "   ------------------- -------------------- 6.2/12.8 MB 1.2 MB/s eta 0:00:06\n",
      "   ------------------- -------------------- 6.3/12.8 MB 1.2 MB/s eta 0:00:06\n",
      "   ------------------- -------------------- 6.4/12.8 MB 1.2 MB/s eta 0:00:06\n",
      "   -------------------- ------------------- 6.4/12.8 MB 1.2 MB/s eta 0:00:06\n",
      "   -------------------- ------------------- 6.5/12.8 MB 1.2 MB/s eta 0:00:06\n",
      "   -------------------- ------------------- 6.5/12.8 MB 1.2 MB/s eta 0:00:06\n",
      "   -------------------- ------------------- 6.6/12.8 MB 1.2 MB/s eta 0:00:06\n",
      "   -------------------- ------------------- 6.6/12.8 MB 1.2 MB/s eta 0:00:06\n",
      "   -------------------- ------------------- 6.6/12.8 MB 1.2 MB/s eta 0:00:06\n",
      "   -------------------- ------------------- 6.6/12.8 MB 1.2 MB/s eta 0:00:06\n",
      "   -------------------- ------------------- 6.6/12.8 MB 1.2 MB/s eta 0:00:06\n",
      "   --------------------- ------------------ 6.7/12.8 MB 1.1 MB/s eta 0:00:06\n",
      "   --------------------- ------------------ 6.8/12.8 MB 1.1 MB/s eta 0:00:06\n",
      "   --------------------- ------------------ 6.9/12.8 MB 1.1 MB/s eta 0:00:06\n",
      "   --------------------- ------------------ 6.9/12.8 MB 1.1 MB/s eta 0:00:06\n",
      "   --------------------- ------------------ 7.0/12.8 MB 1.2 MB/s eta 0:00:06\n",
      "   ---------------------- ----------------- 7.0/12.8 MB 1.2 MB/s eta 0:00:05\n",
      "   ---------------------- ----------------- 7.1/12.8 MB 1.2 MB/s eta 0:00:05\n",
      "   ---------------------- ----------------- 7.2/12.8 MB 1.2 MB/s eta 0:00:05\n",
      "   ---------------------- ----------------- 7.2/12.8 MB 1.2 MB/s eta 0:00:05\n",
      "   ---------------------- ----------------- 7.3/12.8 MB 1.2 MB/s eta 0:00:05\n",
      "   ----------------------- ---------------- 7.4/12.8 MB 1.2 MB/s eta 0:00:05\n",
      "   ----------------------- ---------------- 7.4/12.8 MB 1.2 MB/s eta 0:00:05\n",
      "   ----------------------- ---------------- 7.5/12.8 MB 1.2 MB/s eta 0:00:05\n",
      "   ----------------------- ---------------- 7.6/12.8 MB 1.2 MB/s eta 0:00:05\n",
      "   ----------------------- ---------------- 7.7/12.8 MB 1.2 MB/s eta 0:00:05\n",
      "   ------------------------ --------------- 7.8/12.8 MB 1.2 MB/s eta 0:00:05\n",
      "   ------------------------ --------------- 7.8/12.8 MB 1.2 MB/s eta 0:00:05\n",
      "   ------------------------ --------------- 7.8/12.8 MB 1.2 MB/s eta 0:00:05\n",
      "   ------------------------ --------------- 7.9/12.8 MB 1.2 MB/s eta 0:00:05\n",
      "   ------------------------ --------------- 8.0/12.8 MB 1.2 MB/s eta 0:00:05\n",
      "   ------------------------- -------------- 8.0/12.8 MB 1.2 MB/s eta 0:00:05\n",
      "   ------------------------- -------------- 8.1/12.8 MB 1.2 MB/s eta 0:00:04\n",
      "   ------------------------- -------------- 8.2/12.8 MB 1.2 MB/s eta 0:00:04\n",
      "   ------------------------- -------------- 8.2/12.8 MB 1.2 MB/s eta 0:00:04\n",
      "   ------------------------- -------------- 8.3/12.8 MB 1.2 MB/s eta 0:00:04\n",
      "   -------------------------- ------------- 8.3/12.8 MB 1.2 MB/s eta 0:00:04\n",
      "   -------------------------- ------------- 8.4/12.8 MB 1.2 MB/s eta 0:00:04\n",
      "   -------------------------- ------------- 8.5/12.8 MB 1.2 MB/s eta 0:00:04\n",
      "   -------------------------- ------------- 8.5/12.8 MB 1.2 MB/s eta 0:00:04\n",
      "   -------------------------- ------------- 8.6/12.8 MB 1.2 MB/s eta 0:00:04\n",
      "   --------------------------- ------------ 8.6/12.8 MB 1.2 MB/s eta 0:00:04\n",
      "   --------------------------- ------------ 8.7/12.8 MB 1.2 MB/s eta 0:00:04\n",
      "   --------------------------- ------------ 8.8/12.8 MB 1.2 MB/s eta 0:00:04\n",
      "   --------------------------- ------------ 8.8/12.8 MB 1.2 MB/s eta 0:00:04\n",
      "   --------------------------- ------------ 8.9/12.8 MB 1.2 MB/s eta 0:00:04\n",
      "   --------------------------- ------------ 8.9/12.8 MB 1.2 MB/s eta 0:00:04\n",
      "   ---------------------------- ----------- 9.0/12.8 MB 1.2 MB/s eta 0:00:04\n",
      "   ---------------------------- ----------- 9.1/12.8 MB 1.2 MB/s eta 0:00:04\n",
      "   ---------------------------- ----------- 9.1/12.8 MB 1.2 MB/s eta 0:00:04\n",
      "   ---------------------------- ----------- 9.2/12.8 MB 1.2 MB/s eta 0:00:04\n",
      "   ---------------------------- ----------- 9.2/12.8 MB 1.2 MB/s eta 0:00:03\n",
      "   ---------------------------- ----------- 9.3/12.8 MB 1.2 MB/s eta 0:00:03\n",
      "   ----------------------------- ---------- 9.3/12.8 MB 1.2 MB/s eta 0:00:03\n",
      "   ----------------------------- ---------- 9.4/12.8 MB 1.2 MB/s eta 0:00:03\n",
      "   ----------------------------- ---------- 9.5/12.8 MB 1.2 MB/s eta 0:00:03\n",
      "   ----------------------------- ---------- 9.5/12.8 MB 1.2 MB/s eta 0:00:03\n",
      "   ------------------------------ --------- 9.6/12.8 MB 1.2 MB/s eta 0:00:03\n",
      "   ------------------------------ --------- 9.7/12.8 MB 1.2 MB/s eta 0:00:03\n",
      "   ------------------------------ --------- 9.7/12.8 MB 1.2 MB/s eta 0:00:03\n",
      "   ------------------------------ --------- 9.8/12.8 MB 1.2 MB/s eta 0:00:03\n",
      "   ------------------------------ --------- 9.9/12.8 MB 1.2 MB/s eta 0:00:03\n",
      "   ------------------------------ --------- 9.9/12.8 MB 1.2 MB/s eta 0:00:03\n",
      "   ------------------------------- -------- 10.0/12.8 MB 1.2 MB/s eta 0:00:03\n",
      "   ------------------------------- -------- 10.0/12.8 MB 1.2 MB/s eta 0:00:03\n",
      "   ------------------------------- -------- 10.1/12.8 MB 1.2 MB/s eta 0:00:03\n",
      "   ------------------------------- -------- 10.2/12.8 MB 1.2 MB/s eta 0:00:03\n",
      "   -------------------------------- ------- 10.3/12.8 MB 1.2 MB/s eta 0:00:03\n",
      "   -------------------------------- ------- 10.3/12.8 MB 1.2 MB/s eta 0:00:03\n",
      "   -------------------------------- ------- 10.4/12.8 MB 1.2 MB/s eta 0:00:03\n",
      "   -------------------------------- ------- 10.4/12.8 MB 1.3 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 10.5/12.8 MB 1.2 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 10.6/12.8 MB 1.2 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 10.6/12.8 MB 1.2 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 10.7/12.8 MB 1.2 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 10.7/12.8 MB 1.2 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 10.8/12.8 MB 1.2 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 10.9/12.8 MB 1.2 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 10.9/12.8 MB 1.2 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 11.0/12.8 MB 1.2 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 11.0/12.8 MB 1.2 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 11.1/12.8 MB 1.2 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 11.2/12.8 MB 1.2 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 11.2/12.8 MB 1.3 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 11.3/12.8 MB 1.3 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 11.3/12.8 MB 1.3 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 11.3/12.8 MB 1.3 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 11.4/12.8 MB 1.3 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 11.5/12.8 MB 1.3 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 11.6/12.8 MB 1.3 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 11.6/12.8 MB 1.3 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 11.7/12.8 MB 1.3 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 11.8/12.8 MB 1.3 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 11.8/12.8 MB 1.3 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 11.9/12.8 MB 1.3 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 11.9/12.8 MB 1.3 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 12.0/12.8 MB 1.3 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 12.1/12.8 MB 1.3 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 12.1/12.8 MB 1.3 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 12.2/12.8 MB 1.3 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 12.3/12.8 MB 1.3 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 12.3/12.8 MB 1.3 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 12.3/12.8 MB 1.3 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 12.3/12.8 MB 1.3 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 12.4/12.8 MB 1.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  12.5/12.8 MB 1.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  12.5/12.8 MB 1.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  12.6/12.8 MB 1.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  12.6/12.8 MB 1.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  12.7/12.8 MB 1.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  12.7/12.8 MB 1.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  12.8/12.8 MB 1.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  12.8/12.8 MB 1.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  12.8/12.8 MB 1.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 12.8/12.8 MB 1.3 MB/s eta 0:00:00\n",
      "Using cached setuptools-80.9.0-py3-none-any.whl (1.2 MB)\n",
      "Using cached MarkupSafe-3.0.2-cp312-cp312-win_amd64.whl (15 kB)\n",
      "Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Installing collected packages: mpmath, typing-extensions, sympy, setuptools, pillow, numpy, networkx, MarkupSafe, fsspec, filelock, jinja2, torch, torchvision, torchaudio\n",
      "Successfully installed MarkupSafe-3.0.2 filelock-3.19.1 fsspec-2025.9.0 jinja2-3.1.6 mpmath-1.3.0 networkx-3.5 numpy-2.3.3 pillow-11.3.0 setuptools-80.9.0 sympy-1.14.0 torch-2.8.0 torchaudio-2.8.0 torchvision-0.23.0 typing-extensions-4.15.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "! pip3 install torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ffb6d82f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "class GPTDataset(Dataset):\n",
    "    def __init__(self, txt, tokenizer, max_length, stride):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "        # Step 1 - we will tokenize the entire text\n",
    "        token_ids = tokenizer.encode(txt,allowed_special={\"<|endofText|>\"})\n",
    "        # Step 2 - we will use sliding window to chunk the book into overlapping sequences\n",
    "        for i in range(0, len(token_ids)-max_length, stride):\n",
    "            input_chunk = token_ids[i:i+max_length]\n",
    "            target_chunk = token_ids[i+1:i+max_length+1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "\n",
    "    # Step 3 - Then return the total no. of rows\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "    \n",
    "    # Step 4 - Return a single row from the datset\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3d59421a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we have to create the data loader\n",
    "def create_dataloader(txt, batch_size=4, max_length=256, stride=128, shuffle=True, drop_last=True, num_workers=0):\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "    dataset = GPTDataset(txt, tokenizer, max_length, stride)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last, num_workers=num_workers)\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67aefa23",
   "metadata": {},
   "source": [
    "Now Let us Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8359a2c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"01 Harry Potter and the Sorcerers Stone.txt\", \"r\", encoding=\"Utf-8\") as f:\n",
    "    raw_text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "298db16b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[ 44, 374,  13, 290]]), tensor([[ 374,   13,  290, 9074]])]\n"
     ]
    }
   ],
   "source": [
    "# Now I am gonna convert dataloader into a python iterator to fetch the next entry\n",
    "\n",
    "import torch\n",
    "dataloader = create_dataloader(raw_text, batch_size=1, max_length=4, stride=1, shuffle=False)\n",
    "iter_data = iter(dataloader)\n",
    "first_batch = next(iter_data)\n",
    "print(first_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f12bd7e",
   "metadata": {},
   "source": [
    "see as we used stride=1 so it will overlap so doing this will make the model overfit and noisy .Lets see if we use stride=4 and increase batch_size to 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "149690f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Inputs:\n",
      " tensor([[   44,   374,    13,   290],\n",
      "        [ 9074,    13,   360,  1834],\n",
      "        [ 1636,    11,   286,  1271],\n",
      "        [ 1440,    11,  4389, 16809],\n",
      "        [ 9974,    11,   547,  6613],\n",
      "        [  284,   910,   326,   484],\n",
      "        [  547,  7138,  3487,    11],\n",
      "        [ 5875,   345,   845,   881]])\n",
      "\n",
      "Targets:\n",
      " tensor([[  374,    13,   290,  9074],\n",
      "        [   13,   360,  1834,  1636],\n",
      "        [   11,   286,  1271,  1440],\n",
      "        [   11,  4389, 16809,  9974],\n",
      "        [   11,   547,  6613,   284],\n",
      "        [  910,   326,   484,   547],\n",
      "        [ 7138,  3487,    11,  5875],\n",
      "        [  345,   845,   881,    13]])\n"
     ]
    }
   ],
   "source": [
    "dataloader = create_dataloader(raw_text, batch_size=8, max_length=4, stride=4, shuffle=False)\n",
    "iter_data = iter(dataloader)\n",
    "inputs, targets = next(iter_data)\n",
    "print(\"\\nInputs:\\n\", inputs)\n",
    "print(\"\\nTargets:\\n\", targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b823922",
   "metadata": {},
   "source": [
    "batch size = 8 means the input tensors are also 8 rows"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_tokenizer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
