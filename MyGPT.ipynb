{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3acbbf66",
   "metadata": {},
   "source": [
    "I am building my own LLM so first I am building a tokenizer of my own "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c0e579",
   "metadata": {},
   "source": [
    "STEP 1 - CREATE THE TOKENS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d4f192a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "439478\n",
      "M r. and Mrs. Dursley, of number four, Privet Drive, were proud to say that they were perfectly nor\n"
     ]
    }
   ],
   "source": [
    "with open(\"01 Harry Potter and the Sorcerers Stone.txt\", \"r\", encoding=\"Utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "\n",
    "print(len(raw_text)) #Total number of characters\n",
    "print(raw_text[:99])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "573c7f0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['M', 'r', '.', 'and', 'Mrs', '.', 'Dursley', ',', 'of', 'number', 'four', ',', 'Privet', 'Drive', ',', 'were', 'proud', 'to', 'say', 'that', 'they', 'were', 'perfectly', 'normal', ',', 'thank', 'you', 'very', 'much', '.']\n"
     ]
    }
   ],
   "source": [
    "import re  #library for splitting\n",
    "preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', raw_text)\n",
    "preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "print(preprocessed[:30]) # Remember this is a list having all the tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "680d5641",
   "metadata": {},
   "source": [
    "STEP 2 - CREATE TOKEN ID FOR THIS TOKENS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3565b607",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7572\n"
     ]
    }
   ],
   "source": [
    "#now we have to use vocabulary that is the sorted words\n",
    "words = sorted(set(preprocessed))\n",
    "vocab_size = len(words)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e6aeb2f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remember vocab is a mapping from tokens to tokenIDs\n",
    "vocab = {token:integer for integer, token in enumerate(words)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eed5f7af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('!', 0)\n",
      "(\"'\", 1)\n",
      "('(', 2)\n",
      "(')', 3)\n",
      "('*', 4)\n",
      "(',', 5)\n",
      "('-', 6)\n",
      "('-bodied', 7)\n",
      "('.', 8)\n",
      "('1', 9)\n",
      "('1473', 10)\n",
      "('1637', 11)\n",
      "('17', 12)\n",
      "('1709', 13)\n",
      "('1945', 14)\n",
      "('2', 15)\n",
      "('3', 16)\n",
      "('31', 17)\n",
      "('382', 18)\n",
      "('4', 19)\n",
      "(':', 20)\n",
      "(';', 21)\n",
      "('?', 22)\n",
      "('A', 23)\n",
      "('ALBUS', 24)\n",
      "('ALLEY', 25)\n",
      "('ALLOWED', 26)\n",
      "('AM', 27)\n",
      "('AND', 28)\n",
      "('ANYTHING', 29)\n",
      "('ARE', 30)\n",
      "('AT', 31)\n",
      "('About', 32)\n",
      "('According', 33)\n",
      "('Adalbert', 34)\n",
      "('Add', 35)\n",
      "('Adrian', 36)\n",
      "('Africa', 37)\n",
      "('African', 38)\n",
      "('After', 39)\n",
      "('Against', 40)\n",
      "('Ages', 41)\n",
      "('Agrippa', 42)\n",
      "('Ah', 43)\n",
      "('Ahead', 44)\n",
      "('Alberic', 45)\n",
      "('Albus', 46)\n",
      "('Albus…”', 47)\n",
      "('Algie', 48)\n",
      "('Alicia', 49)\n",
      "('All', 50)\n"
     ]
    }
   ],
   "source": [
    "#enumerate - this I have used to assign integer values to the sorted words\n",
    "for i, item in enumerate(vocab.items()):\n",
    "    print(item)\n",
    "    if i>=50:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c31949",
   "metadata": {},
   "source": [
    "WE NEED TO REMEMBER TWO CONCEPTS i.e. ENCODER AND DECODER\n",
    "\n",
    "ENCODER -> it will take text as input and give token ID as output\n",
    "\n",
    "DECODER -> it will take token ID as input and will give text as output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bd5e32b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizer:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = {i:s for s,i in vocab.items()}\n",
    "\n",
    "    def encode(self, text):\n",
    "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "        ids = [self.str_to_int[s] for s in preprocessed]\n",
    "        return ids\n",
    "    \n",
    "    def decode(self, ids):\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8e76bf6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[593, 6597, 4470, 6131, 2269, 4452, 6131, 5888, 6126, 3488, 4427]\n"
     ]
    }
   ],
   "source": [
    "#Now let us create an object of the above class and check if it is returning the ids or not\n",
    "tokenizer = SimpleTokenizer(vocab)\n",
    "text =  \"\"\"It was on the corner of the street that he noticed\"\"\"\n",
    "ids = tokenizer.encode(text)\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b714b33d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'It was on the corner of the street that he noticed'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a10f8e8f",
   "metadata": {},
   "source": [
    "So encoder and decoder is perfectly working as expected"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f5ffb6d",
   "metadata": {},
   "source": [
    "NOW WE HAVE COMPLETED A SIMPLE TOKENIZER OF OUR OWN BUT THERE ARE SOME LIMITATIONS. WE CANNOT USE OTHER TEXT OTHER THAN THE ONE IN THE txt file WE USED MEANING IF I WRITE SOME TEXT THERE WILL BE ERROR SO WE NEED SPECIAL CONTEXT TOKENS WHICH IS ALSO USED BY GPT MODELS .\n",
    "\n",
    "<|endofText|> and <|unk|> -> unknown will be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9ed60075",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7574"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = sorted(list(set(preprocessed)))\n",
    "tokens.extend([\"<|endofText|>\", \"<|unk|>\"])# Here I used extend it will add 2 extra entries see the result it is 7574 but previously it was 7572\n",
    "vocab = {token:integer for integer, token in enumerate(tokens)}\n",
    "len(vocab.items())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40cc2815",
   "metadata": {},
   "source": [
    "Let us check "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "94ef89f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('…Oak', 7569)\n",
      "('…Then', 7570)\n",
      "('…”', 7571)\n",
      "('<|endofText|>', 7572)\n",
      "('<|unk|>', 7573)\n"
     ]
    }
   ],
   "source": [
    "for i, item in enumerate(list(vocab.items())[-5:]):\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7789a176",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdvancedTokenizer:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = {i:s for s,i in vocab.items()}\n",
    "\n",
    "    def encode(self,text):\n",
    "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "        preprocessed = [item if item in self.str_to_int else \"<|unk|>\" for item in preprocessed]\n",
    "        ids = [self.str_to_int[s] for s in preprocessed]\n",
    "        return ids\n",
    "    \n",
    "    def decode(self, ids):\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7743c899",
   "metadata": {},
   "source": [
    "Now let us try again with some random text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "649f76ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, my name is Subhranil Mondal <|endofText|> I am from Kolkata.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AdvancedTokenizer(vocab)\n",
    "\n",
    "text1 = \"Hello, my name is Subhranil Mondal\"\n",
    "text2 = \"I am from Kolkata.\"\n",
    "\n",
    "text = \" <|endofText|> \".join((text1,text2))\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eecb8b0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[7573, 5, 4318, 4332, 3758, 7573, 7573, 7572, 575, 1391, 3164, 7573, 8]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "98eb0bf4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|unk|>, my name is <|unk|> <|unk|> <|endofText|> I am from <|unk|>.'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenizer.encode(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ac3d5b3",
   "metadata": {},
   "source": [
    "See this is a word based tokenizer but GPT uses subword tokenizer so for that we need Byte Pair encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9ad115da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tiktoken in c:\\mygpt\\llm_tokenizer\\lib\\site-packages (0.11.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\mygpt\\llm_tokenizer\\lib\\site-packages (from tiktoken) (2025.9.18)\n",
      "Requirement already satisfied: requests>=2.26.0 in c:\\mygpt\\llm_tokenizer\\lib\\site-packages (from tiktoken) (2.32.5)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\mygpt\\llm_tokenizer\\lib\\site-packages (from requests>=2.26.0->tiktoken) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\mygpt\\llm_tokenizer\\lib\\site-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\mygpt\\llm_tokenizer\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\mygpt\\llm_tokenizer\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2025.8.3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "#we will be using the BPE tokenizer which OpenAI uses\n",
    "! pip3 install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dc29e1d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a0c006a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fcea8358",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15496, 11, 616, 1438, 318, 3834, 16848, 346, 27328, 282, 13, 314, 716, 422, 509, 13597, 1045, 13]\n"
     ]
    }
   ],
   "source": [
    "#Let us test\n",
    "\n",
    "text = \"Hello, my name is Subhranil Mondal. I am from Kolkata.\"\n",
    "integers = tokenizer.encode(text, allowed_special={\"<|endofText|>\"})\n",
    "print(integers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "43fdd490",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, my name is Subhranil Mondal. I am from Kolkata.\n"
     ]
    }
   ],
   "source": [
    "s = tokenizer.decode(integers)\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46533c8f",
   "metadata": {},
   "source": [
    "Therefore encoder and decoder is working pretty well"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa711ab",
   "metadata": {},
   "source": [
    "Now, we have to create Input-Target pairs\n",
    "But what is it?\n",
    "see we need to do self supervised learning kind of unsupervised. The data are not labeled here . LLM will be feed one word and it will predict next one This is what we gonna implement which the GPT does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5f9941da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "124336\n"
     ]
    }
   ],
   "source": [
    "with open(\"01 Harry Potter and the Sorcerers Stone.txt\", \"r\", encoding=\"Utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "\n",
    "encoded_text = tokenizer.encode(raw_text)\n",
    "print(len(encoded_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7eafa68b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = encoded_text[50:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "65bf5d7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: [11428, 11, 780, 484]\n",
      "y: [11, 780, 484, 655]\n"
     ]
    }
   ],
   "source": [
    "conSize = 4 #context size -> length of input sequence\n",
    "x = sample[:conSize]\n",
    "y = sample[1:conSize+1]\n",
    "print(f\"x: {x}\")\n",
    "print(f\"y: {y}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c77846c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11428] -> 11\n",
      "[11428, 11] -> 780\n",
      "[11428, 11, 780] -> 484\n",
      "[11428, 11, 780, 484] -> 655\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, conSize+1):\n",
    "    context = sample[:i]\n",
    "    desired = sample[i]\n",
    "\n",
    "    print(context, \"->\", desired)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd311566",
   "metadata": {},
   "source": [
    "Now just decode it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ec7b11f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " mysterious -> ,\n",
      " mysterious, ->  because\n",
      " mysterious, because ->  they\n",
      " mysterious, because they ->  just\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, conSize+1):\n",
    "    context = sample[:i]\n",
    "    desired = sample[i]\n",
    "\n",
    "    print(tokenizer.decode(context), \"->\", tokenizer.decode([desired]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d0b53d3",
   "metadata": {},
   "source": [
    "so now we have to turn tokens into embeddings and implement a data loader as inputs and PyTorch Tensors as target like you can think about multidimensional arrays."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "366e7dcd",
   "metadata": {},
   "source": [
    "STEP 3 - IMPLEMENT A DATALOADER(WE WILL BE USING PYTORCH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9934607a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\mygpt\\llm_tokenizer\\lib\\site-packages (2.8.0)\n",
      "Requirement already satisfied: torchvision in c:\\mygpt\\llm_tokenizer\\lib\\site-packages (0.23.0)\n",
      "Requirement already satisfied: torchaudio in c:\\mygpt\\llm_tokenizer\\lib\\site-packages (2.8.0)\n",
      "Requirement already satisfied: filelock in c:\\mygpt\\llm_tokenizer\\lib\\site-packages (from torch) (3.19.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\mygpt\\llm_tokenizer\\lib\\site-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\mygpt\\llm_tokenizer\\lib\\site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\mygpt\\llm_tokenizer\\lib\\site-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in c:\\mygpt\\llm_tokenizer\\lib\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in c:\\mygpt\\llm_tokenizer\\lib\\site-packages (from torch) (2025.9.0)\n",
      "Requirement already satisfied: setuptools in c:\\mygpt\\llm_tokenizer\\lib\\site-packages (from torch) (80.9.0)\n",
      "Requirement already satisfied: numpy in c:\\mygpt\\llm_tokenizer\\lib\\site-packages (from torchvision) (2.3.3)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\mygpt\\llm_tokenizer\\lib\\site-packages (from torchvision) (11.3.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\mygpt\\llm_tokenizer\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\mygpt\\llm_tokenizer\\lib\\site-packages (from jinja2->torch) (3.0.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "! pip3 install torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ffb6d82f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "class GPTDataset(Dataset):\n",
    "    def __init__(self, txt, tokenizer, max_length, stride):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "        # Step 1 - we will tokenize the entire text\n",
    "        token_ids = tokenizer.encode(txt,allowed_special={\"<|endofText|>\"})\n",
    "        # Step 2 - we will use sliding window to chunk the book into overlapping sequences\n",
    "        for i in range(0, len(token_ids)-max_length, stride):\n",
    "            input_chunk = token_ids[i:i+max_length]\n",
    "            target_chunk = token_ids[i+1:i+max_length+1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "\n",
    "    # Step 3 - Then return the total no. of rows\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "    \n",
    "    # Step 4 - Return a single row from the datset\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3d59421a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we have to create the data loader\n",
    "def create_dataloader(txt, batch_size=4, max_length=256, stride=128, shuffle=True, drop_last=True, num_workers=0):\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "    dataset = GPTDataset(txt, tokenizer, max_length, stride)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last, num_workers=num_workers)\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67aefa23",
   "metadata": {},
   "source": [
    "Now Let us Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8359a2c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"01 Harry Potter and the Sorcerers Stone.txt\", \"r\", encoding=\"Utf-8\") as f:\n",
    "    raw_text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "298db16b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[ 44, 374,  13, 290]]), tensor([[ 374,   13,  290, 9074]])]\n"
     ]
    }
   ],
   "source": [
    "# Now I am gonna convert dataloader into a python iterator to fetch the next entry\n",
    "\n",
    "import torch\n",
    "dataloader = create_dataloader(raw_text, batch_size=1, max_length=4, stride=1, shuffle=False)\n",
    "iter_data = iter(dataloader)\n",
    "first_batch = next(iter_data)\n",
    "print(first_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f12bd7e",
   "metadata": {},
   "source": [
    "see as we used stride=1 so it will overlap so doing this will make the model overfit and noisy .Lets see if we use stride=4 and increase batch_size to 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "149690f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Inputs:\n",
      " tensor([[   44,   374,    13,   290],\n",
      "        [ 9074,    13,   360,  1834],\n",
      "        [ 1636,    11,   286,  1271],\n",
      "        [ 1440,    11,  4389, 16809],\n",
      "        [ 9974,    11,   547,  6613],\n",
      "        [  284,   910,   326,   484],\n",
      "        [  547,  7138,  3487,    11],\n",
      "        [ 5875,   345,   845,   881]])\n",
      "\n",
      "Targets:\n",
      " tensor([[  374,    13,   290,  9074],\n",
      "        [   13,   360,  1834,  1636],\n",
      "        [   11,   286,  1271,  1440],\n",
      "        [   11,  4389, 16809,  9974],\n",
      "        [   11,   547,  6613,   284],\n",
      "        [  910,   326,   484,   547],\n",
      "        [ 7138,  3487,    11,  5875],\n",
      "        [  345,   845,   881,    13]])\n"
     ]
    }
   ],
   "source": [
    "dataloader = create_dataloader(raw_text, batch_size=8, max_length=4, stride=4, shuffle=False)\n",
    "iter_data = iter(dataloader)\n",
    "inputs, targets = next(iter_data)\n",
    "print(\"\\nInputs:\\n\", inputs)\n",
    "print(\"\\nTargets:\\n\", targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b823922",
   "metadata": {},
   "source": [
    "batch size = 8 means the input tensors are also 8 rows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f29312b3",
   "metadata": {},
   "source": [
    "Now let us create token or vector embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3888d6f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = torch.tensor([2,3,5,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c0e95d",
   "metadata": {},
   "source": [
    "FOR SIMPLICITY WE WILL USE VOCAB SIZE 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5e93b24a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.3374, -0.1778, -0.1690],\n",
      "        [ 0.9178,  1.5810,  1.3010],\n",
      "        [ 1.2753, -0.2010, -0.1606],\n",
      "        [-0.4015,  0.9666, -1.1481],\n",
      "        [-1.1589,  0.3255, -0.6315],\n",
      "        [-2.8400, -0.7849, -1.4096]], requires_grad=True)\n",
      "tensor([[ 1.2753, -0.2010, -0.1606],\n",
      "        [-0.4015,  0.9666, -1.1481],\n",
      "        [-2.8400, -0.7849, -1.4096],\n",
      "        [ 0.9178,  1.5810,  1.3010]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 6\n",
    "output_dim = 3 # Output dimensions\n",
    "# Output will be a 6 X 3 matrix\n",
    "torch.manual_seed(123)\n",
    "embedding_layer = torch.nn.Embedding(vocab_size, output_dim)\n",
    "print(embedding_layer.weight)\n",
    "print(embedding_layer(input_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4496c9f",
   "metadata": {},
   "source": [
    "Now see this is just a basic implementation of token embedding but this is not the right way. we meed to have the concept of positional embeddings -> absolute and relative. GPT or any other transfromer uses absolute embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1ddd7671",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 50257 #Lets give the actual vocab_size for GPT2\n",
    "output_dim = 256 #Lets keep it 256\n",
    "token_embedding_layer = torch.nn.Embedding(vocab_size, output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "df9e3ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now let us create the data loader again\n",
    "max_length = 4\n",
    "dataloader = create_dataloader(raw_text, batch_size=8, max_length=max_length, stride=max_length, shuffle=False)\n",
    "data_iter = iter(dataloader)\n",
    "inputs, targets = next(data_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cecb3ae9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Input IDs:\n",
      " tensor([[   44,   374,    13,   290],\n",
      "        [ 9074,    13,   360,  1834],\n",
      "        [ 1636,    11,   286,  1271],\n",
      "        [ 1440,    11,  4389, 16809],\n",
      "        [ 9974,    11,   547,  6613],\n",
      "        [  284,   910,   326,   484],\n",
      "        [  547,  7138,  3487,    11],\n",
      "        [ 5875,   345,   845,   881]])\n",
      "\n",
      "Target IDs:\n",
      " tensor([[  374,    13,   290,  9074],\n",
      "        [   13,   360,  1834,  1636],\n",
      "        [   11,   286,  1271,  1440],\n",
      "        [   11,  4389, 16809,  9974],\n",
      "        [   11,   547,  6613,   284],\n",
      "        [  910,   326,   484,   547],\n",
      "        [ 7138,  3487,    11,  5875],\n",
      "        [  345,   845,   881,    13]])\n",
      "\n",
      "Inputs shape: torch.Size([8, 4])\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nInput IDs:\\n\", inputs)\n",
    "print(\"\\nTarget IDs:\\n\", targets)\n",
    "print(\"\\nInputs shape:\", inputs.shape) # 8 x 4 tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a87375fa",
   "metadata": {},
   "source": [
    "Each token ID is now embedded as a 256- dimensional vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b2250a7",
   "metadata": {},
   "source": [
    "Now let us create an embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "32cb861e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 4, 256])\n"
     ]
    }
   ],
   "source": [
    "token_embeddings = token_embedding_layer(inputs)\n",
    "print(token_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd0f717",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_length = max_length\n",
    "# no.of rows = context_length and no.of columns = output_dim\n",
    "pos_embedding_layer = torch.nn.Embedding(context_length, output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d801bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 256])\n"
     ]
    }
   ],
   "source": [
    "pos_embeddings = pos_embedding_layer(torch.arange(max_length))\n",
    "#Here torch.arange is creating a sequence of numbers from 0 to max_length-1\n",
    "print(pos_embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb6d5d15",
   "metadata": {},
   "source": [
    "Now let us add the 4x256 dimensional tensor to each 4x256 token embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "433f220f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 4, 256])\n"
     ]
    }
   ],
   "source": [
    "input_embeddings = token_embeddings + pos_embeddings\n",
    "print(input_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de7d0853",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_tokenizer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
