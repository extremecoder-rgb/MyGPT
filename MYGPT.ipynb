{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3acbbf66",
   "metadata": {},
   "source": [
    "I am building my own LLM so first I am building a tokenizer of my own "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c0e579",
   "metadata": {},
   "source": [
    "STEP 1 - CREATE THE TOKENS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d4f192a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "439478\n",
      "M r. and Mrs. Dursley, of number four, Privet Drive, were proud to say that they were perfectly nor\n"
     ]
    }
   ],
   "source": [
    "with open(\"01 Harry Potter and the Sorcerers Stone.txt\", \"r\", encoding=\"Utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "\n",
    "print(len(raw_text)) #Total number of characters\n",
    "print(raw_text[:99])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "573c7f0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['M', 'r', '.', 'and', 'Mrs', '.', 'Dursley', ',', 'of', 'number', 'four', ',', 'Privet', 'Drive', ',', 'were', 'proud', 'to', 'say', 'that', 'they', 'were', 'perfectly', 'normal', ',', 'thank', 'you', 'very', 'much', '.']\n"
     ]
    }
   ],
   "source": [
    "import re  #library for splitting\n",
    "preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', raw_text)\n",
    "preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "print(preprocessed[:30]) # Remember this is a list having all the tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "680d5641",
   "metadata": {},
   "source": [
    "STEP 2 - CREATE TOKEN ID FOR THIS TOKENS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3565b607",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7572\n"
     ]
    }
   ],
   "source": [
    "#now we have to use vocabulary that is the sorted words\n",
    "words = sorted(set(preprocessed))\n",
    "vocab_size = len(words)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6aeb2f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remember vocab is a mapping from tokens to tokenIDs\n",
    "vocab = {token:integer for integer, token in enumerate(words)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed5f7af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('!', 0)\n",
      "(\"'\", 1)\n",
      "('(', 2)\n",
      "(')', 3)\n",
      "('*', 4)\n",
      "(',', 5)\n",
      "('-', 6)\n",
      "('-bodied', 7)\n",
      "('.', 8)\n",
      "('1', 9)\n",
      "('1473', 10)\n",
      "('1637', 11)\n",
      "('17', 12)\n",
      "('1709', 13)\n",
      "('1945', 14)\n",
      "('2', 15)\n",
      "('3', 16)\n",
      "('31', 17)\n",
      "('382', 18)\n",
      "('4', 19)\n",
      "(':', 20)\n",
      "(';', 21)\n",
      "('?', 22)\n",
      "('A', 23)\n",
      "('ALBUS', 24)\n",
      "('ALLEY', 25)\n",
      "('ALLOWED', 26)\n",
      "('AM', 27)\n",
      "('AND', 28)\n",
      "('ANYTHING', 29)\n",
      "('ARE', 30)\n",
      "('AT', 31)\n",
      "('About', 32)\n",
      "('According', 33)\n",
      "('Adalbert', 34)\n",
      "('Add', 35)\n",
      "('Adrian', 36)\n",
      "('Africa', 37)\n",
      "('African', 38)\n",
      "('After', 39)\n",
      "('Against', 40)\n",
      "('Ages', 41)\n",
      "('Agrippa', 42)\n",
      "('Ah', 43)\n",
      "('Ahead', 44)\n",
      "('Alberic', 45)\n",
      "('Albus', 46)\n",
      "('Albus…”', 47)\n",
      "('Algie', 48)\n",
      "('Alicia', 49)\n",
      "('All', 50)\n"
     ]
    }
   ],
   "source": [
    "#enumerate - this I have used to assign integer values to the sorted words\n",
    "for i, item in enumerate(vocab.items()):\n",
    "    print(item)\n",
    "    if i>=50:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c31949",
   "metadata": {},
   "source": [
    "WE NEED TO REMEMBER TWO CONCEPTS i.e. ENCODER AND DECODER\n",
    "\n",
    "ENCODER -> it will take text as input and give token ID as output\n",
    "\n",
    "DECODER -> it will take token ID as input and will give text as output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bd5e32b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizer:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = {i:s for s,i in vocab.items()}\n",
    "\n",
    "    def encode(self, text):\n",
    "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "        ids = [self.str_to_int[s] for s in preprocessed]\n",
    "        return ids\n",
    "    \n",
    "    def decode(self, ids):\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8e76bf6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[593, 6597, 4470, 6131, 2269, 4452, 6131, 5888, 6126, 3488, 4427]\n"
     ]
    }
   ],
   "source": [
    "#Now let us create an object of the above class and check if it is returning the ids or not\n",
    "tokenizer = SimpleTokenizer(vocab)\n",
    "text =  \"\"\"It was on the corner of the street that he noticed\"\"\"\n",
    "ids = tokenizer.encode(text)\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b714b33d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'It was on the corner of the street that he noticed'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a10f8e8f",
   "metadata": {},
   "source": [
    "So encoder and decoder is perfectly working as expected"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f5ffb6d",
   "metadata": {},
   "source": [
    "NOW WE HAVE COMPLETED A SIMPLE TOKENIZER OF OUR OWN BUT THERE ARE SOME LIMITATIONS. WE CANNOT USE OTHER TEXT OTHER THAN THE ONE IN THE txt file WE USED MEANING IF I WRITE SOME TEXT THERE WILL BE ERROR SO WE NEED SPECIAL CONTEXT TOKENS WHICH IS ALSO USED BY GPT MODELS .\n",
    "\n",
    "<|endofText|> and <|unk|> -> unknown will be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed60075",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7574"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = sorted(list(set(preprocessed)))\n",
    "tokens.extend([\"<|endofText|>\", \"<|unk|>\"])# Here I used extend it will add 2 extra entries see the result it is 7574 but previously it was 7572\n",
    "vocab = {token:integer for integer, token in enumerate(tokens)}\n",
    "len(vocab.items())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40cc2815",
   "metadata": {},
   "source": [
    "Let us check "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "94ef89f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('…Oak', 7569)\n",
      "('…Then', 7570)\n",
      "('…”', 7571)\n",
      "('<|endofText|>', 7572)\n",
      "('<|unk|>', 7573)\n"
     ]
    }
   ],
   "source": [
    "for i, item in enumerate(list(vocab.items())[-5:]):\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7789a176",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdvancedTokenizer:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = {i:s for s,i in vocab.items()}\n",
    "\n",
    "    def encode(self,text):\n",
    "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "        preprocessed = [item if item in self.str_to_int else \"<|unk|>\" for item in preprocessed]\n",
    "        ids = [self.str_to_int[s] for s in preprocessed]\n",
    "        return ids\n",
    "    \n",
    "    def decode(self, ids):\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7743c899",
   "metadata": {},
   "source": [
    "Now let us try again with some random text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "649f76ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, my name is Subhranil Mondal <|endofText|> I am from Kolkata.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AdvancedTokenizer(vocab)\n",
    "\n",
    "text1 = \"Hello, my name is Subhranil Mondal\"\n",
    "text2 = \"I am from Kolkata.\"\n",
    "\n",
    "text = \" <|endofText|> \".join((text1,text2))\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "eecb8b0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[7573, 5, 4318, 4332, 3758, 7573, 7573, 7572, 575, 1391, 3164, 7573, 8]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "98eb0bf4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|unk|>, my name is <|unk|> <|unk|> <|endofText|> I am from <|unk|>.'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenizer.encode(text))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_tokenizer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
